{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629936a4",
   "metadata": {},
   "source": [
    "# wav2vec 2.0 Hidden_dropout ablation\n",
    "\n",
    "## Placeholders introduced\n",
    "- `PATH_TO_DATASET` — path_to_dataset \n",
    "- `PATH_TO_OUTPUT_DIR` -path_to_output\n",
    "- `PATH_TO_REPO` -path_to_base_repo\n",
    "- `PARAMS` -different parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ec357-ffd4-4a18-972d-49a6033115f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "reqs=['transformers', 'datasets', 'soundfile', 'librosa', 'evaluate', 'jiwer', 'accelerate','pandas','wandb']\n",
    "def check_installed(packages):\n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            importlib.import_module(pkg)\n",
    "            print(f\"{pkg} is installed\")\n",
    "        except ImportError:\n",
    "            print(f\"{pkg} is NOT installed\")\n",
    "check_installed(reqs)\n",
    "\n",
    "import torch, transformers\n",
    "print(f\"PyTorch: {torch.__version__}\")  \n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "\n",
    "# DTensor check\n",
    "try:\n",
    "    from torch.distributed.tensor import DTensor\n",
    "    print(\" DTensor available\")\n",
    "except ImportError:\n",
    "    raise RuntimeError(\" DTensor not found - upgrade PyTorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a054fb-431e-4e4d-bb4b-7038c4c224b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union\n",
    "import evaluate \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import re\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9d409-dddf-417e-b611-067b3884e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"PATH_TO_DATASET/processed_json/combined_dataset.json\"\n",
    "AUDIO_BASE_PATH = \"PATH_TO_DATASET/extracted/audio_files\"\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "print(f\"Loading dataset from: {json_path}\")\n",
    "dataset = load_dataset(\"json\", data_files=json_path, split=\"train\")\n",
    "print(f\"Loaded dataset with {len(dataset)} samples\")\n",
    "\n",
    "print(\"Normalizing absolute paths to real audio files...\")\n",
    "\n",
    "def normalize_audio_path(example):\n",
    "    relative_filename = example[\"audio\"].split(\"clips/\")[-1]\n",
    "    example[\"audio\"] = os.path.join(AUDIO_BASE_PATH, \"clips\", relative_filename)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(normalize_audio_path)\n",
    "\n",
    "data = [{\"audio\": x[\"audio\"], \"sentence\": x[\"transcription\"]} for x in dataset]\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "print(\"Audio decoding complete!\")\n",
    "print(\"Sample entry:\", train_dataset[0])\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\" Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5073294-6be9-4f81-b8ce-ee441058ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_special_unwanted_characters = [\n",
    "    'ఁ', 'ౄ', 'ౢ', 'ౣ', 'ౠ', 'ఽ',\n",
    "    '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯',\n",
    "    'ఀ', 'ౘ', 'ౙ', 'ౚ', '౷',\n",
    "    '‘', '’', '“', '”', '%', '.', ';', '-', ',', '/', '\\\\', '_', '&',\n",
    "    'G', 'P', 'S', 'e', 'l', 'n', 'r', 't', '\\u200c', '\\n'\n",
    "]\n",
    "\n",
    "chars_to_remove_regex = f\"[{re.escape(''.join(telugu_special_unwanted_characters))}]\"\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"])\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(remove_special_characters)\n",
    "val_dataset   = val_dataset.map(remove_special_characters)\n",
    "\n",
    "print(\"Special characters removed from 'sentence' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3035e0-5a50-4f08-8729-cc9537aad5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repo_name = \"PATH_TO_REPO\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_repo_name)\n",
    "model = AutoModelForCTC.from_pretrained(base_repo_name).to(\"cuda\")\n",
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9738518-43aa-490a-b46b-74fc1e9df73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "    \n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "print(\"Datasets prepared and tokenized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43f080-8acd-4ead-a3db-2a761762fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(\n",
    "    processor=processor,\n",
    "    padding=True  \n",
    ")\n",
    "print(\"Data collator ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8225b8-fd3d-43ae-95ef-fba598eda7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"*****\") #<insert hugging face hub login token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5372d7-a071-4a8d-86d2-688e38b85881",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "    return {\n",
    "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
    "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b663010-119a-4cba-a717-9eead0ae24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412f6d3-f21d-4ae6-a839-58ba9b89903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hidden_dropout(hidden_dropout, run_id):\n",
    "    print(f\"Training with hidden dropout: {hidden_dropout}\")\n",
    "    \n",
    "    model = AutoModelForCTC.from_pretrained(base_repo_name).to(\"cuda\")\n",
    "    model.freeze_feature_encoder()\n",
    "    model.config.hidden_dropout = hidden_dropout\n",
    "    \n",
    "    print(\" Updated classifier dropout to:\", hidden_dropout)\n",
    "    print(\" New hidden dropout:\", model.config.hidden_dropout)\n",
    "    output_dir = f\"PATH_TO_OUTPUT_DIR/hidden_dropout_{hidden_dropout}\"\n",
    "    repo_name = f\"PATH_TO_REPO/hidden_dropout_{hidden_dropout}\"\n",
    "    PARAMS = {\n",
    "        \"learning_rate\":5e-5,\n",
    "        \"epochs\": 15,\n",
    "        \"batch_size\": 8,\n",
    "        \"sampling_rate\": 16000\n",
    "    } #replace with necessary parameter values\n",
    "    wandb.init(\n",
    "        project=\"telugu-asr-wav2vec_ablation\",\n",
    "        name=f\"dropoutablation_{hidden_dropout}\",\n",
    "        config={\n",
    "            \"learning_rate\": 5e-5, \n",
    "            \"hidden_dropout\": hidden_dropout,\n",
    "            \"freeze_feature_encoder\": True,\n",
    "            \"base_model\": base_repo_name\n",
    "        }\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=PARAMS[\"learning_rate\"],\n",
    "        per_device_train_batch_size=PARAMS[\"batch_size\"],\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=700,\n",
    "        logging_steps=50,\n",
    "        save_steps=1500,\n",
    "        num_train_epochs=PARAMS[\"epochs\"],\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        warmup_ratio=0.1,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"dropout_ablation_{hidden_dropout}\",\n",
    "        push_to_hub=True,\n",
    "        hub_model_id=repo_name,\n",
    "        logging_dir=f\"{output_dir}/logs\"\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=processor,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub(commit_message=f\"Trained with hidden dropout {hidden_dropout}\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecde16b-7b39-420f-b791-ecc8b70fff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_hidden_dropout(0.2, \"hd_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56ba14-0bc2-454b-9b8b-2ceb6dd81888",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_hidden_dropout(0.3, \"hd_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641dbb2d-878e-4b12-81f7-e0a44c24d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def evaluate_model(repo_name: str, test_dataset):\n",
    "    print(f\"\\nEvaluating: {repo_name}\")\n",
    "    processor = AutoProcessor.from_pretrained(repo_name)\n",
    "    model = AutoModelForCTC.from_pretrained(repo_name).to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    def map_to_prediction(batch):\n",
    "        with torch.no_grad():\n",
    "            input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "            logits = model(input_values).logits\n",
    "            pred_ids = torch.argmax(logits, dim=-1)\n",
    "            batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "            batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "        return batch\n",
    "\n",
    "    results = test_dataset.map(map_to_prediction, remove_columns=test_dataset.column_names)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])\n",
    "    cer = cer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])\n",
    "    \n",
    "    print(f\"WER for {repo_name}: {wer:.4f}\")\n",
    "    print(f\"CER for {repo_name}: {cer:.4f}\")\n",
    "    \n",
    "    return wer, cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5115d9-4949-4766-8b82-f62e7598a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_names = [\n",
    "    \"PATH_TO_REPO/hidden_dropout_0.2\",\n",
    "    \"PATH_TO_REPO/hidden_dropout_0.3\"\n",
    "]\n",
    "\n",
    "results_dict = {}\n",
    "for repo in repo_names:\n",
    "    wer, cer = evaluate_model(repo, val_dataset) \n",
    "    results_dict[repo] = {\"WER\": wer, \"CER\": cer}\n",
    "\n",
    "for k, v in results_dict.items():\n",
    "    print(f\"Evaluation for {k}:\\nWER: {v['WER']:.4f} | CER: {v['CER']:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
