{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7f5391",
   "metadata": {},
   "source": [
    "# wav2vec 2.0 learning_rate ablation\n",
    "\n",
    "## Placeholders introduced\n",
    "- `PATH_TO_DATASET` — where to place your dataset (e.g., data/ )\n",
    "- `PATH_TO_OUTPUT_DIR` -path_to_output\n",
    "- `PATH_TO_REPO` -path_to_base_repo\n",
    "- `PARAMS` -different parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd13a7-cdcf-46d0-b9b4-2d6a58061232",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets soundfile librosa evaluate jiwer kaggle accelerate\n",
    "!pip install pandas\n",
    "!pip install huggingface_hub\n",
    "!pip install wandb\n",
    "!pip install transformers\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e79c0-799b-4354-87d0-1b56ca85a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "reqs=['transformers', 'datasets', 'soundfile', 'librosa', 'evaluate', 'jiwer', 'accelerate','pandas','wandb']\n",
    "def check_installed(packages):\n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            importlib.import_module(pkg)\n",
    "            print(f\"{pkg} is installed\")\n",
    "        except ImportError:\n",
    "            print(f\"{pkg} is NOT installed\")\n",
    "check_installed(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e210e5-e7e4-404b-949a-b34f08bcf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers\n",
    "print(f\"PyTorch: {torch.__version__}\")  \n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "\n",
    "# DTensor check\n",
    "try:\n",
    "    from torch.distributed.tensor import DTensor\n",
    "    print(\" DTensor available\")\n",
    "except ImportError:\n",
    "    raise RuntimeError(\" DTensor not found - upgrade PyTorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260a415-9375-4884-aae3-6d80be0e6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union\n",
    "import evaluate \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import re\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf991ce-e303-48d9-9c7c-272706829e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"PATH_TO_DATASET/processed_json/combined_dataset.json\"\n",
    "AUDIO_BASE_PATH = \"PATH_TO_DATASET/extracted/audio_files\"\n",
    "SAMPLING_RATE = 16000\n",
    "print(f\"Loading dataset from: {json_path}\")\n",
    "dataset = load_dataset(\"json\", data_files=json_path, split=\"train\")\n",
    "print(f\"Loaded dataset with {len(dataset)} samples\")\n",
    "\n",
    "print(\"Normalizing absolute paths to real audio files...\")\n",
    "\n",
    "def normalize_audio_path(example):\n",
    "    relative_filename = example[\"audio\"].split(\"clips/\")[-1]\n",
    "    example[\"audio\"] = os.path.join(AUDIO_BASE_PATH, \"clips\", relative_filename)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(normalize_audio_path)\n",
    "\n",
    "data = [{\"audio\": x[\"audio\"], \"sentence\": x[\"transcription\"]} for x in dataset]\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "print(\"Audio decoding complete!\")\n",
    "print(\"Sample entry:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cc30b-ca08-4245-a0bf-e99b50033315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\" Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73bd6d-99aa-4d75-8b7f-81441dfd9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_special_unwanted_characters = [\n",
    "    'ఁ', 'ౄ', 'ౢ', 'ౣ', 'ౠ', 'ఽ',\n",
    "    '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯',\n",
    "    'ఀ', 'ౘ', 'ౙ', 'ౚ', '౷',\n",
    "    '‘', '’', '“', '”', '%', '.', ';', '-', ',', '/', '\\\\', '_', '&',\n",
    "    'G', 'P', 'S', 'e', 'l', 'n', 'r', 't', '\\u200c', '\\n'\n",
    "]\n",
    "\n",
    "chars_to_remove_regex = f\"[{re.escape(''.join(telugu_special_unwanted_characters))}]\"\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"])\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(remove_special_characters)\n",
    "val_dataset   = val_dataset.map(remove_special_characters)\n",
    "\n",
    "print(\"Special characters removed from 'sentence' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffff85-b89a-4d81-ae39-ec5873f2fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repo_name = \"PATH_TO_REPO\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_repo_name)\n",
    "model = AutoModelForCTC.from_pretrained(base_repo_name).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8cc465-bb0d-45c6-9351-ab42debeac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea30e9-b82e-4933-9f16-206909de1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "    \n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "print(\"Datasets prepared and tokenized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac9884-8402-4f55-9b11-99726547fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prpeare data collator\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(\n",
    "    processor=processor,\n",
    "    padding=True  \n",
    ")\n",
    "print(\"Data collator ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f16b0-5d56-4d5b-b9f8-a2bd4fe6606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"******\") #insert token here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925178d0-997a-48bf-b397-3fd266b79df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "    return {\n",
    "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
    "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS={'EPOCHS':15, #Enter required epoch value\n",
    "         'HIDDEN_DROPOUT': 0.1 #Enter required hidden dropout value\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe419d1-b3ef-42b8-adad-6f82288acf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4b88c-fa94-4a60-b4ec-ddeebab22c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr(learning_rate, run_id):\n",
    "    print(f\"Training with learning rate: {learning_rate}\")\n",
    "    model = AutoModelForCTC.from_pretrained(base_repo_name).to(\"cuda\")\n",
    "    model.freeze_feature_encoder()\n",
    "    output_dir = f\"PATH_TO_OUTPUT_DIR/lr_ablation_{run_id}\"\n",
    "    repo_name = f\"PATH_TO_REPO/lr_ablation_{run_id}\"\n",
    "    PARAMS = {\n",
    "        \"epochs\": PARAMS['EPOCHS'],\n",
    "        \"batch_size\": 8,\n",
    "        \"hidden_dropout\": PARAMS['HIDDEN DROPOUT'],\n",
    "        \"sampling_rate\": 16000\n",
    "    }\n",
    "    wandb.init(\n",
    "    project=\"telugu-asr-wav2vec_ablation\", \n",
    "    name=f\"lr_ablation_{learning_rate}\", \n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"freeze_feature_encoder\": True,\n",
    "        \"base_model\": \"finetunedxlsr-300m\"\n",
    "    }\n",
    "    )\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        group_by_length=True,\n",
    "        per_device_train_batch_size=PARAMS[\"batch_size\"],\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_strategy=\"steps\",\n",
    "        num_train_epochs=PARAMS[\"epochs\"],\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,  \n",
    "        save_steps=200,\n",
    "        eval_steps=200,\n",
    "        logging_steps=50,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=0.1,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"lr_ablation_{learning_rate}\",\n",
    "        push_to_hub=True,\n",
    "        hub_model_id=repo_name,\n",
    "        logging_dir=f\"{output_dir}/logs\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=processor,\n",
    "        data_collator=data_collator, \n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub(commit_message=f\"Trained with learning rate {learning_rate}\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97f06a-eb5d-42ad-9aa0-faa9f29fc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_lr(learning_rate=1e-4, run_id=\"1e-4\")\n",
    "train_with_lr(learning_rate=5e-5, run_id=\"5e-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37f48b-9293-470a-a11f-d418f1902254",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def evaluate_model(repo_name: str, test_dataset):\n",
    "    print(f\"\\nEvaluating: {repo_name}\")\n",
    "    processor = AutoProcessor.from_pretrained(repo_name)\n",
    "    model = AutoModelForCTC.from_pretrained(repo_name).to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    def map_to_prediction(batch):\n",
    "        with torch.no_grad():\n",
    "            input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "            logits = model(input_values).logits\n",
    "            pred_ids = torch.argmax(logits, dim=-1)\n",
    "            batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "            batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "        return batch\n",
    "\n",
    "    results = test_dataset.map(map_to_prediction, remove_columns=test_dataset.column_names)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])\n",
    "    cer = cer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])\n",
    "    \n",
    "    print(f\"WER for {repo_name}: {wer:.4f}\")\n",
    "    print(f\"CER for {repo_name}: {cer:.4f}\")\n",
    "    \n",
    "    return wer, cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad0a54-c542-4ef8-83f5-8cf44f1fe152",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_names = [\n",
    "    \"PATH_TO_NEW_REPO1\",\n",
    "    \"PATH_TO_NEW_REPO2\"\n",
    "]\n",
    "\n",
    "results_dict = {}\n",
    "for repo in repo_names:\n",
    "    wer, cer = evaluate_model(repo, val_dataset)\n",
    "    results_dict[repo] = {\"WER\": wer, \"CER\": cer}\n",
    "for k,v in results_dict.items():\n",
    "    print(f\"output of {k}:\\n{v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
