{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19e0f87",
   "metadata": {},
   "source": [
    "# wav2vec 2.0 finetuning \n",
    "\n",
    "## Table of contents\n",
    "1. [Environment & Imports](#Environment-&-Imports)\n",
    "2. [Dataset & Paths](#Dataset-&-Paths)\n",
    "4. [PreProcessing and Model Definition](#Model-Definition)\n",
    "5. [Training](#Training)\n",
    "6. [Evaluation](#Evaluation)\n",
    "\n",
    "## Placeholders introduced\n",
    "- `PATH_TO_DATASET` — where to place your dataset (e.g., data/ )\n",
    "- `PATH_TO_CHECKPOINT` -outputs/logs\n",
    "- `PATH_TO_MODEL` — Saved model, vocabulary\n",
    "- `KAGGLE_DATASET_LINK` — redacted private Kaggle link\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad479eef",
   "metadata": {},
   "source": [
    "## Environment & Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9968b8-5e2e-4f6b-8333-fe83d0fb449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets soundfile librosa evaluate jiwer kaggle pandas wandb\n",
    "!pip install huggingface_hub\n",
    "!pip install transformers accelerate\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26ac70-2638-4692-a142-daddc756b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "\n",
    "try:\n",
    "    from torch.distributed.tensor import DTensor\n",
    "    print(\" DTensor available\")\n",
    "except ImportError:\n",
    "    raise RuntimeError(\" DTensor not found - upgrade PyTorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf8457-cbf9-4874-9c22-d333b65a29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version (torch):\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483c037-22d8-4284-bd5a-8f8986f592e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, accelerate\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec1d09-b3df-4a61-9405-843bf2760cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "reqs=['transformers', 'datasets', 'soundfile', 'librosa', 'evaluate', 'jiwer', 'accelerate','pandas','wandb']\n",
    "def check_installed(packages):\n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            importlib.import_module(pkg)\n",
    "            print(f\"{pkg} is installed\")\n",
    "        except ImportError:\n",
    "            print(f\"{pkg} is NOT installed\")\n",
    "check_installed(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All necessary imports\n",
    "\n",
    "import os, json, numpy as np, pandas as pd, random,re\n",
    "import zipfile\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, Audio, load_dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union\n",
    "from transformers import EarlyStoppingCallback\n",
    "from huggingface_hub import HfApi\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "from jiwer import wer, cer\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8d1eb",
   "metadata": {},
   "source": [
    "## Datasets and Paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0a434-aec4-4a0a-b48f-b41548f37ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up storage directories\n",
    "os.makedirs(\"PATH_TO_DATASET\", exist_ok=True)       # Store original dataset\n",
    "os.makedirs(\"PATH_TO_MODEL\", exist_ok=True)         # Final models, tokenizer, processor\n",
    "\n",
    "os.makedirs(\"PATH_TO_CHECKPOINT\", exist_ok=True)         # Trainer checkpoints\n",
    "os.makedirs(\"PATH_TO_LOGS\", exist_ok=True)                # Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d23ee-b580-4b94-9e11-c17279dacaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"*****\"\n",
    "os.environ['KAGGLE_KEY'] = \"*****\"\n",
    "!kaggle datasets download -d KAGGLE_DATASET_LINK -p PATH_TO_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88376c1-eea1-4608-973f-4eee9f9867fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"PATH_TO_DATASET/your-dataset-name.zip\"\n",
    "extract_path = \"PATH_TO_DATASET/extracted\"\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Extracted to: {extract_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ceb885-bb80-4794-ae44-86abbe650c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To verify by printing transcriptions file\n",
    "\n",
    "csv_path = \"PATH_TO_DATASET/extracted/transcriptions.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ed41e-2c4d-4600-9ca2-5053159ec4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR= Path(\"PATH_TO_MODEL/trained_model\")  # Final trained model\n",
    "\n",
    "for path in [SAVE_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b46117",
   "metadata": {},
   "source": [
    "## Preprocessing and Model Definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70acbd6-4821-4d64-b4a2-bddbfa32d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_base_path = \"PATH_TO_DATASET/extracted/audio_files\"\n",
    "json_output_path = \"PATH_TO_DATASET/processed_json\"\n",
    "os.makedirs(json_output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "#preparing the JSON file\n",
    "\n",
    "json_data = []\n",
    "for _, row in df.iterrows():\n",
    "    entry = {\n",
    "        \"audio\": os.path.join(audio_base_path, row[\"filename\"]),  # fixed key here\n",
    "        \"transcription\": str(row[\"transcription\"]).strip()\n",
    "    }\n",
    "    json_data.append(entry)\n",
    "\n",
    "json_save_path = os.path.join(json_output_path, \"combined_dataset.json\")\n",
    "with open(json_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in json_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\" Saved combined JSON dataset to: {json_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581f0a8-8be9-406a-8aaa-a9c34b9b057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"PATH_TO_DATASET/processed_json/combined_dataset.json\" \n",
    "\n",
    "print(\"Previewing first 3 entries in the JSON file:\")\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(3):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        sample = json.loads(line)\n",
    "        print(f\"Sample {i+1}: {sample}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843b716-38f6-4199-aa64-e5ec77cbf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audio decoding\n",
    "audio_base_path = \"PATH_TO_DATASET/extracted/audio_files\"\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "print(f\"Loading dataset from: {json_path}\")\n",
    "dataset = load_dataset(\"json\", data_files=json_path, split=\"train\")\n",
    "print(f\"Loaded dataset with {len(dataset)} samples\")\n",
    "\n",
    "def normalize_audio_path(example):\n",
    "    relative_filename = example[\"audio\"].split(\"/\")[-1]\n",
    "    example[\"audio\"] = os.path.join(audio_base_path, \"clips\", relative_filename)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(normalize_audio_path)\n",
    "train_val_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "telugu_special_unwanted_characters = [\n",
    "    'ఁ', 'ౄ', 'ౢ', 'ౣ', 'ౠ', 'ఽ',\n",
    "    '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯',\n",
    "    'ఀ', 'ౘ', 'ౙ', 'ౚ', '౷',\n",
    "    '‘', '’', '“', '”', '%', '.', ';', '-', ',', '/', '\\\\', '_', '&',\n",
    "    'G', 'P', 'S', 'e', 'l', 'n', 'r', 't', '\\u200c', '\\n'\n",
    "]\n",
    "\n",
    "chars_to_remove_regex = f\"[{re.escape(''.join(telugu_special_unwanted_characters))}]\"\n",
    "def clean_and_rename_text(batch):\n",
    "    cleaned_text = re.sub(chars_to_remove_regex, '', batch[\"transcription\"])\n",
    "    batch[\"sentence\"] = cleaned_text.strip()\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(clean_and_rename_text, remove_columns=[\"transcription\"])\n",
    "val_dataset = val_dataset.map(clean_and_rename_text, remove_columns=[\"transcription\"])\n",
    "\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(\"Audio decoding complete!\")\n",
    "print(\"Sample entry:\", train_dataset[0]) #to verify compatiblity with datasets library\n",
    "\n",
    "def get_all_text(dataset):\n",
    "    return \" \".join(dataset[\"sentence\"])\n",
    "\n",
    "all_text = get_all_text(train_dataset) + get_all_text(val_dataset)\n",
    "vocab_chars = sorted(set(all_text))\n",
    "vocab_dict = {char: idx for idx, char in enumerate(vocab_chars)}\n",
    "\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "print(f\"Clean vocab size: {len(vocab_dict)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a0315-5bd6-4977-9d0f-5765d8329fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"*****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907aab8c-3fad-4b93-824d-daa26b685d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "repo_name = \"PATH_TO_REPO\"   #Enter model repo \n",
    "api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
    "print(f\"Created (or found) repo: {repo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec3404-23a3-4626-a525-596b8aedf94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_DIR = Path(\"PATH_TO_MODEL/Vocab\")\n",
    "VOCAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VOCAB_FILE = VOCAB_DIR / \"vocab.json\"\n",
    "repo_name= \"PATH_TO_REPO\"\n",
    "with open(VOCAB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False)\n",
    "print(f\"Vocab saved to: {VOCAB_FILE}\")\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_file=str(VOCAB_FILE),\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\"\n",
    ")\n",
    "print(\" Tokenizer created.\")\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "print(\" Feature extractor created.\")\n",
    "\n",
    "processor = Wav2Vec2Processor(\n",
    "    tokenizer=tokenizer,\n",
    "    feature_extractor=feature_extractor\n",
    ")\n",
    "\n",
    "processor.save_pretrained(str(VOCAB_DIR))\n",
    "print(f\"Processor saved locally at: {VOCAB_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36148032-e96c-49e6-9aa2-d123fe61df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2Processor\n",
    "vocab_dir = \"PATH_TO_MODEL/Vocab\"\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(repo_name)\n",
    "tokenizer.save_pretrained(vocab_dir)\n",
    "print(\"Tokenizer loaded from Hub\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(vocab_dir)\n",
    "print(\"Processor loaded from local directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc489475-170e-4a4b-b8cc-cfd6bab85a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = Path(\"PATH_TO_MODEL/Vocab/vocab.json\")\n",
    "\n",
    "with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3de21d",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2531ef-db81-46f0-9b5f-494a68855a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "    \n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "print(\"Datasets prepared and tokenized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3fb32-8531-4a66-b296-1dfb267513af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data collator\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(\n",
    "    processor=processor,\n",
    "    padding=True  \n",
    ")\n",
    "print(\"Data collator ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59ffb1-7342-4ba5-8841-6c28f0c4c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "    return {\n",
    "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
    "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83edb9e-c56d-4459-9ad9-6acb76307041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\", #replace with required model \n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True, \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer) \n",
    ")\n",
    "model.freeze_feature_encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0dd680-46c1-4e2b-aecf-07247d16964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 8,\n",
    "    \"hidden_dropout\": 0.1,\n",
    "    \"sampling_rate\": 16000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70208b1d-8236-4ae3-b3bd-18616a0b8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb logging\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"WANDB_PROJECT_NAME\", name=\"xlsr-300m-medium\")\n",
    "OUTPUT_DIR = \"PATH_TO_CHECKPOINT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eecb00-766f-4215-917a-6cf77e864994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=PARAMS[\"batch_size\"],\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=PARAMS[\"epochs\"],\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,  \n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=PARAMS[\"learning_rate\"],\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\", \n",
    "    push_to_hub=True,\n",
    "    hub_model_id=repo_name,\n",
    "    disable_tqdm=False,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\"\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e117b1b-ae62-495b-baf0-806b9d00ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3651cb9-f740-4ac8-8704-02e7f1890dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6d2d5-e607-4e01-b7d9-53b6c2b90b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(str(SAVE_DIR))\n",
    "print(f\"Processor saved locally at: {SAVE_DIR}\")\n",
    "model.save_pretrained(str(SAVE_DIR))\n",
    "print(f\"Model saved locally at: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff7889-2267-45e2-b33c-3f78f5da07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921efb2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa012c0-1382-4dfb-b661-68b68219de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(repo_name)\n",
    "model = AutoModelForCTC.from_pretrained(repo_name).to(\"cuda\")\n",
    "def map_to_prediction(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "        batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "    return batch\n",
    "\n",
    "results = val_dataset.map(map_to_prediction, remove_columns=val_dataset.column_names)\n",
    "\n",
    "fW2 = wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])\n",
    "fC2= cer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9903d-5361-43b3-8e1c-f5d0479e93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFinal Test WER: {fW2:.4f}\")\n",
    "print(f\"Final Test CER: {fC2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
