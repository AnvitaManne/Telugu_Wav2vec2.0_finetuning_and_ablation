{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1acc4ec",
   "metadata": {},
   "source": [
    "# wav2vec 2.0 optimizer ablation\n",
    "\n",
    "## Placeholders introduced\n",
    "- `PATH_TO_DATASET` — where to place your dataset (e.g., data/ )\n",
    "- `PATH_TO_OUTPUT_DIR` -path_to_output\n",
    "- `PATH_TO_REPO` -path_to_base_repo\n",
    "- `PARAMS` -different parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862770b-a6af-4acb-be21-35a3b07d0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --upgrade\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
    "!pip install datasets soundfile librosa evaluate jiwer accelerate\n",
    "%pip install pandas\n",
    "!pip install huggingface_hub\n",
    "!pip install wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6d2ed-90c8-4076-a9dd-67401d8586cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "reqs=['transformers', 'datasets', 'soundfile', 'librosa', 'evaluate', 'jiwer', 'accelerate','pandas','wandb']\n",
    "def check_installed(packages):\n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            importlib.import_module(pkg)\n",
    "            print(f\"{pkg} is installed\")\n",
    "        except ImportError:\n",
    "            print(f\"{pkg} is NOT installed\")\n",
    "check_installed(reqs)\n",
    "import torch, transformers\n",
    "print(f\"PyTorch: {torch.__version__}\")  # Must show 2.1.0+\n",
    "print(f\"Transformers: {transformers.__version__}\")  # 4.36.0+\n",
    "\n",
    "try:\n",
    "    from torch.distributed.tensor import DTensor\n",
    "    print(\" DTensor available\")\n",
    "except ImportError:\n",
    "    raise RuntimeError(\" DTensor not found - upgrade PyTorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179289c5-5802-4e45-a67f-dc639b9e2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_scheduler,\n",
    "    Auto_processor,\n",
    "    AutoModelForCTC\n",
    ")\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union\n",
    "import evaluate \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import re\n",
    "from huggingface_hub import login\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers.optimization import Adafactor\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "import evaluate\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b82c1-c526-4814-bbb6-f635ca1f4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651d712-49d4-4751-81b7-ef56839dbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"PATH_TO_DATASET/processed_json/combined_dataset.json\"\n",
    "AUDIO_BASE_PATH = \"PATH_TO_DATASET/extracted/audio_files\"\n",
    "SAMPLING_RATE = 16000\n",
    "print(f\"Loading dataset from: {json_path}\")\n",
    "dataset = load_dataset(\"json\", data_files=json_path, split=\"train\")\n",
    "print(f\"Loaded dataset with {len(dataset)} samples\")\n",
    "\n",
    "print(\"Normalizing absolute paths to real audio files...\")\n",
    "\n",
    "def normalize_audio_path(example):\n",
    "    relative_filename = example[\"audio\"].split(\"clips/\")[-1]\n",
    "    example[\"audio\"] = os.path.join(AUDIO_BASE_PATH, \"clips\", relative_filename)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(normalize_audio_path)\n",
    "\n",
    "data = [{\"audio\": x[\"audio\"], \"sentence\": x[\"transcription\"]} for x in dataset]\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "print(\"Audio decoding complete!\")\n",
    "print(\"Sample entry:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96331675-7ad3-4908-a02b-90c6bb1ee990",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\" Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f57d5-edbe-421e-a894-e1a990b05f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_special_unwanted_characters = [\n",
    "    'ఁ', 'ౄ', 'ౢ', 'ౣ', 'ౠ', 'ఽ',\n",
    "    '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯',\n",
    "    'ఀ', 'ౘ', 'ౙ', 'ౚ', '౷',\n",
    "    '‘', '’', '“', '”', '%', '.', ';', '-', ',', '/', '\\\\', '_', '&',\n",
    "    'G', 'P', 'S', 'e', 'l', 'n', 'r', 't', '\\u200c', '\\n'\n",
    "]\n",
    "\n",
    "chars_to_remove_regex = f\"[{re.escape(''.join(telugu_special_unwanted_characters))}]\"\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"])\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(remove_special_characters)\n",
    "val_dataset   = val_dataset.map(remove_special_characters)\n",
    "\n",
    "print(\"Special characters removed from 'sentence' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca6a76-983a-45fc-876e-db0c2d7cd4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repo_name = \"PATH_TO_REPO\"\n",
    "processor = AutoProcessor.from_pretrained(base_repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adb097-fb16-45f7-90bf-8f8513ba913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "    \n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "print(\"Datasets prepared and tokenized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0ce63-435e-4fe6-b739-7344d6ae4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(\n",
    "    processor=processor,\n",
    "    padding=True  \n",
    ")\n",
    "print(\"Data collator ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efaee9a-77d1-437f-8cf9-c2d1abedc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"***************\") #insert your hugging face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5bbaec-a14c-41a7-a2c7-679ef1a8a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "    return {\n",
    "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
    "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e108d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS={\n",
    "  \"learning_rate\":1e-4,  #Enter required learning rate value\n",
    "  \"epoch\":15, #enter required epoch value\n",
    "  \"hidden_dropout\":0.3 #enter required hidden dropout value\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01116e-06d2-403a-b4a3-54c53a6047ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForCTC, get_scheduler\n",
    "from torch.optim import AdamW, Adam \n",
    "from transformers.optimization import Adafactor \n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_with_optimizer(opt_name, run_id):\n",
    "    print(f\"Training with optimizer: {opt_name}\")\n",
    "    \n",
    "    model = AutoModelForCTC.from_pretrained(base_repo_name).to(\"cuda\")\n",
    "    model.freeze_feature_encoder()\n",
    "    model.config.hidden_dropout = PARAMS[\"hidden_dropout\"]\n",
    "    \n",
    "    output_dir = f\"PATH_TO_OUTPUT_DIR/optim_ablation{run_id}\"\n",
    "    repo_name = f\"PATH_TO_REPO/telugu_wav2vec_optimizerablation_{run_id}\"\n",
    "\n",
    "    if opt_name == \"adamw\":\n",
    "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    elif opt_name == \"adam\":\n",
    "        optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "    elif opt_name == \"adafactor\":\n",
    "        optimizer = Adafactor(\n",
    "            model.parameters(),\n",
    "            scale_parameter=True,\n",
    "            relative_step=False,  \n",
    "            warmup_init=False,\n",
    "            lr=LEARNING_RATE\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {opt_name}\")\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"telugu-asr-wav2vec_ablation\", \n",
    "        name=f\"optimizer_ablation_{opt_name}\", \n",
    "        config={\"optimizer\": opt_name}\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        group_by_length=True,\n",
    "        learning_rate=PARAMS[\"learning_rate\"],\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_strategy=\"epoch\",\n",
    "        num_train_epochs=PARAMS[\"epoch\"],\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"optimizer_ablation_{opt_name}\",\n",
    "        push_to_hub=True,\n",
    "        hub_model_id=repo_name,\n",
    "        logging_dir=f\"{output_dir}/logs\"\n",
    "    )\n",
    "\n",
    "    num_training_steps = (\n",
    "        len(train_dataset) // training_args.per_device_train_batch_size\n",
    "        * training_args.num_train_epochs\n",
    "    )\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=int(0.1 * num_training_steps),\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=processor,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, lr_scheduler)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub(commit_message=f\"Trained with optimizer: {opt_name}\")\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf63f3-ea36-4bd9-a009-eba55d5b62e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_with_optimizer(\"adamw\", \"adamw\")\n",
    "train_with_optimizer(\"adafactor\", \"adafactor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8cea6-a8f4-4772-b4f5-71b576e4fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "repo_names = [\n",
    "    \"PATH_TO_NEW_REPO_1\",\n",
    "    \"PATH_TO_NEW_REPO_2\"\n",
    "]\n",
    "def evaluate_model(repo_name, dataset):\n",
    "    print(f\" Evaluating {repo_name}\")\n",
    "    processor = AutoProcessor.from_pretrained(repo_name)\n",
    "    model = AutoModelForCTC.from_pretrained(repo_name).to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    def map_to_prediction(batch):\n",
    "        with torch.no_grad():\n",
    "            input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "            logits = model(input_values).logits\n",
    "            pred_ids = torch.argmax(logits, dim=-1)\n",
    "            batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "            batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "        return batch\n",
    "\n",
    "    results = dataset.map(map_to_prediction, remove_columns=dataset.column_names)\n",
    "    wer = wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])\n",
    "    cer = cer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])\n",
    "    return wer, cer\n",
    "\n",
    "results_dict = {}\n",
    "for repo in repo_names:\n",
    "    wer, cer = evaluate_model(repo, val_dataset)\n",
    "    results_dict[repo] = {\"WER\": wer, \"CER\": cer}\n",
    "\n",
    "for model_name, metrics in results_dict.items():\n",
    "    print(f\"\\nResults for {model_name}:\\nWER: {metrics['WER']:.4f} | CER: {metrics['CER']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
